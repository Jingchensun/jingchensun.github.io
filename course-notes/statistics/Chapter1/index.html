<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Chapter 1: Data and Probability Review</title>
  <link rel="stylesheet" href="../../stylesheet.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    h1 {
      text-align: center;
      color: #527bbd;
      font-size: 165%;
      margin-bottom: 40px;
      border-bottom: none;
    }

    h2 {
      color: #2471A3;
      font-size: 125%;
      margin-top: 2em;
    }

    p, ul, li {
      font-size: 16px;
      line-height: 1.6;
    }

    hr {
      border: none;
      border-top: 1px solid #ccc;
      margin: 40px 0;
    }
  </style>
</head>

<body>
<div id="layout-content" style="max-width: 960px; margin: 30px auto; padding: 0 50px;">

  <h1>Chapter 1: Data and Case Studies</h1>

  <h2>Section 1.1: Introduction to Data</h2>
  <p>Statistics is driven by <strong>real applications</strong>:</p>
  <ul>
    <li>Collect data</li>
    <li>Understand what it means</li>
  </ul>
  <p><strong>Example (Sec 1.1: Flight Delays)</strong>:</p>
  <ul>
    <li>Columns = variables</li>
    <li>Rows = observations</li>
  </ul>

  <hr>

  <h2>Section 1.2: Types of Data</h2>
  <ul>
    <li><strong>Population</strong>: Data for all individuals or observations (finite or infinite)</li>
    <li><strong>Sample</strong>: Data from a subset of the population</li>
    <li><strong>Random sample</strong>: Observations are chosen randomly</li>
  </ul>
  <p><strong>Statistical inference</strong>: Making conclusions about a population based on a sample.</p>

  <p><strong>Notation</strong>:</p>
  <ul>
    <li>Capital letters \( X, Y, Z \): random variables</li>
    <li>Lowercase letters \( x_1, x_2, \ldots \): observed values (data)</li>
  </ul>

  <hr>

  <h2>Section 1.3: Observations</h2>
  <h3>Observations of a Random Process</h3>
  <p>Population is infinite. Random sample means independent and identically distributed (i.i.d) observations.</p>
  <p><strong>Example:</strong> Let \( X \in \{0, 1\} \) with equal probability → Bernoulli distribution:</p>
  <p>\( X \sim \text{Bern}(p) \), where:</p>
  <p>\[
    X = \begin{cases}
    1 & \text{with probability } p \\
    0 & \text{with probability } 1 - p
    \end{cases}
  \]</p>

  <h3>Observations of Data</h3>
  <p>Population is finite. May not come from a known random process.</p>
  <p><strong>Example:</strong> Population = \{1, 2, 3, 3̅, 7\} (size \( N = 5 \))</p>
  <ul>
    <li><strong>Sampling with replacement</strong>: sample = \{2, 3, 2\}</li>
    <li><strong>Sampling without replacement</strong>: sample = \{7, 3, 3̅\}</li>
  </ul>
  <p>If \( N \gg n \), both methods yield similar results.</p>

  <hr>

  <h2>Section 1.4: Statistics vs Parameters</h2>
  <ul>
    <li><strong>Statistic</strong>: numerical characteristic of a sample</li>
    <li><strong>Parameter</strong>: numerical characteristic of a population or a probability distribution</li>
  </ul>

  <p><strong>Examples:</strong></p>
  <ul>
    <li>\( X \sim \mathcal{N}(\mu, \sigma^2) \): \( \mu, \sigma \) are parameters</li>
    <li>Sample mean: \( \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i \): statistic (also a random variable)</li>
    <li>Data mean: \( \bar{x} = \frac{1}{n} \sum_{i=1}^n x_i \): statistic</li>
    <li>Population = all babies born in US in 2004:
      <ul>
        <li>Mean of population = parameter \( \mu \)</li>
        <li>Mean of sample of 200 babies = statistic \( \bar{x} \)</li>
      </ul>
    </li>
  </ul>

  <hr>

  <h2>Section 1.5: Surveys</h2>
  <p><strong>Survey</strong>: Ask people what they think or how they live</p>
  <p><strong>Sample survey</strong>: Use a sample from the population due to practicality</p>
  <ul>
    <li>Population → Sampling Frame → Random Sample</li>
  </ul>

  <p><strong>Example:</strong> General Social Survey (GSS)</p>
  <ul>
    <li>Surveys on sociological factors: gender, income, education, etc.</li>
    <li>Archived online since 1970s, evolving questions</li>
    <li>Uses multilevel stratified sampling (not covered in this course)</li>
  </ul>

  <hr>

  <h2>Section 1.6: Observational vs Experimental Studies</h2>
  <p><strong>Observational Study:</strong> Observe only, no intervention</p>
  <p><strong>Example:</strong> Beer and hot wings consumption (Sec. 1.9)</p>

  <p><strong>Experimental Study:</strong> Change conditions or give treatment</p>
  <ul>
    <li>Treatment group vs control group</li>
    <li>Random assignment of subjects to groups</li>
  </ul>

  <p><strong>Example:</strong> Tree seedling growth under different fertilizer/competition (Sec. 1.10)</p>

  <p><strong>Caution:</strong> Non-random samples → results not generalizable</p>
  <p><strong>Example:</strong> Tai Chi arthritis study (Sec. 1.11)</p>
  <ul>
    <li>All volunteers → cannot generalize to broader population</li>
  </ul>
  
  <h1>Probability Review & Statistics </h1>

  <h2>Fundamentals of Probability</h2>
  <p>The <strong>sample space</strong> \( S \) is the set of all possible outcomes of a random experiment.</p>
  <p>Example: Rolling a 6-sided die → \( S = \{1, 2, 3, 4, 5, 6\} \)</p>
  <p>A <strong>discrete random variable</strong> maps outcomes to a countable set: \( X: S \rightarrow \{x_1, x_2, \ldots\} \)</p>
  <p>A <strong>continuous random variable</strong> takes values over the real numbers: \( S = \mathbb{R} \)</p>

  <p><strong>Probability</strong> is the relative frequency of an event if the experiment were repeated many times.</p>
  <p>Example: For a fair die, \( P(1) = \frac{1}{6} \)</p>
  <p>We can write \( P(A) = P(X \in A) \), where \( A \) is a set of outcomes.</p>

  <h3>Conditional Probability</h3>
  <p>\( P(A|B) = \frac{P(A \cap B)}{P(B)} \): the probability of \( A \) given \( B \) has occurred.</p>
  <ul>
    <li>Example: Two dice are rolled</li>
    <li>\( A = \{(1,2), (2,1)\} \), \( P(A) = \frac{2}{36} \)</li>
    <li>\( B = \{(1,1), (1,2), \dots, (1,6)\} \), \( P(B) = \frac{6}{36} \)</li>
    <li>\( A \cap B = \{(1,2)\} \), \( P(A|B) = \frac{1}{6} \)</li>
  </ul>

  <h3>Law of Total Probability</h3>
  <p>If \( \{B_1, B_2, \ldots, B_n\} \) is a partition of \( S \), then:</p>
  <p>\( P(A) = \sum_{i=1}^n P(B_i) P(A|B_i) \)</p>
  <p>Example (rolling a total of 3 with two dice):</p>
  <p>\( P(A) = \frac{1}{6} \cdot \frac{1}{6} + \frac{1}{6} \cdot \frac{1}{6} = \frac{2}{36} \)</p>

  <h3>Discrete Random Variables and PMF</h3>
  <p>A <strong>discrete random variable</strong> has outcomes in a finite or countably infinite set.</p>
  <p>The <strong>probability mass function</strong> (PMF): \( p(x) = P(X = x) \), and \( \sum p(x) = 1 \)</p>
  <p>Example (sum of 2 dice): Distribution peaks at 7 with \( p(7) = \frac{6}{36} \)</p>

  <h3>Binomial Distribution</h3>
  <p>\( p(k) = \binom{n}{k} p^k (1 - p)^{n - k}, \quad k = 0, 1, \ldots, n \)</p>

  <h3>Expected Value and Variance</h3>
  <p>\( \mathbb{E}[g(X)] = \sum_j g(x_j) p(x_j) \)</p>
  <p>\( \mu = \mathbb{E}[X] \), \( \sigma^2 = \mathbb{E}[(X - \mu)^2] \)</p>

  <h3>Continuous Random Variables and PDF</h3>
  <p>PDF: \( f(x) \geq 0 \), \( \int_{-\infty}^{\infty} f(x) dx = 1 \)</p>
  <p>\( P(a < X < b) = \int_a^b f(x) dx \)</p>

  <h3>Cumulative Distribution Function (CDF)</h3>
  <p>\( F(x) = P(X \leq x) = \int_{-\infty}^{x} f(t) dt \)</p>

  <h3>Exponential Distribution</h3>
  <p>\( f(x) = \begin{cases} \lambda e^{-\lambda x}, & x \geq 0 \\ 0, & x < 0 \end{cases} \)</p>
  <p>\( F(x) = 1 - e^{-\lambda x} \)</p>
  <p>Mean: \( \mu = \frac{1}{\lambda} \), Variance: \( \sigma^2 = \frac{1}{\lambda^2} \)</p>

  <h3>Shortcut for Variance</h3>
  <p>\( \text{Var}(X) = \mathbb{E}[X^2] - \mu^2 \)</p>
  <p>\( \text{Var}(a + bX) = b^2 \cdot \text{Var}(X) \)</p>

  <hr>

  <h2>Sample Mean of Random Variables</h2>
  <p>Let \( X_1, X_2, \ldots, X_n \) be independent and identically distributed (i.i.d.) random variables with mean \( \mu \) and variance \( \sigma^2 \).</p>
  <p>The <strong>sample mean</strong> is defined as \( \bar{X} = \frac{1}{n} \sum_{j=1}^{n} X_j \).</p>
  <ul>
    <li>\( \mathbb{E}[\bar{X}] = \mu \)</li>
    <li>\( \text{Var}[\bar{X}] = \frac{\sigma^2}{n} \)</li>
  </ul>

  <p><strong>Example: Bernoulli Random Variable</strong></p>
  <ul>
    <li>\( X_j \in \{0, 1\} \), with \( \mathbb{E}[X_j] = p \), \( \text{Var}[X_j] = p(1 - p) \)</li>
    <li>Then \( \mathbb{E}[\bar{X}] = p \), and \( \text{Var}[\bar{X}] = \frac{p(1 - p)}{n} \)</li>
  </ul>

  <hr>

  <h2>Normal Distribution</h2>
  <p><strong>Normal Distribution:</strong> \( X \sim \mathcal{N}(\mu, \sigma^2) \)</p>
  <p>PDF: \( f(x) = \frac{1}{\sqrt{2\pi} \sigma} e^{- \frac{(x - \mu)^2}{2\sigma^2}} \)</p>

  <p><strong>Standard Normal Distribution:</strong> \( Z \sim \mathcal{N}(0, 1) \)</p>
  <p>PDF: \( f(z) = \frac{1}{\sqrt{2\pi}} e^{- z^2 / 2} \)</p>

  <p><strong>Standardization:</strong> \( Z = \frac{X - \mu}{\sigma}, \quad X = \mu + \sigma Z \)</p>

  <p><strong>Cumulative Distribution Function (CDF):</strong> \( \Phi(z) = \int_{-\infty}^{z} \frac{1}{\sqrt{2\pi}} e^{-t^2 / 2} dt \)</p>

  <p><strong>Approximate Probabilities:</strong></p>
  <ul>
    <li>\( P(-1 < Z < 1) \approx 0.68 \)</li>
    <li>\( P(-2 < Z < 2) \approx 0.95 \)</li>
    <li>\( P(-3 < Z < 3) \approx 0.997 \)</li>
  </ul>

  <p><strong>Sums of Normal Random Variables:</strong> If \( X \sim \mathcal{N}(\mu_1, \sigma_1^2), Y \sim \mathcal{N}(\mu_2, \sigma_2^2) \), and they are independent:</p>
  <p>\( X + Y \sim \mathcal{N}(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2) \)</p>

  <hr>

  <h2>Normal Distribution Application Example</h2>
  <p><strong>Example:</strong> Weight of boys \( X \sim \mathcal{N}(100, 5^2) \), weight of girls \( Y \sim \mathcal{N}(90, 6^2) \)</p>
  <p>Want: \( P(X - Y > 6) \)</p>
  <ul>
    <li>\( X - Y \sim \mathcal{N}(10, 61) \)</li>
    <li>Standardize: \( Z = \frac{(X - Y - 10)}{\sqrt{61}} \)</li>
    <li>\( P(X - Y > 6) = P(Z > -0.5121) \approx 0.695 \)</li>
  </ul>

  <hr>

  <h2>Sum of Independent Normal Variables & Moment Generating Functions</h2>
  <p>Let \( X_i \sim \mathcal{N}(\mu_i, \sigma_i^2) \), and define \( X = \sum a_i X_i \).</p>
  <ul>
    <li>\( \mu = \sum a_i \mu_i \)</li>
    <li>\( \sigma^2 = \sum a_i^2 \sigma_i^2 \)</li>
  </ul>

  <p><strong>Corollary:</strong> If \( X_i \sim \mathcal{N}(\mu_0, \sigma_0^2) \), then:</p>
  <p>\( \bar{X} = \frac{1}{n} \sum X_i \sim \mathcal{N}(\mu_0, \frac{\sigma_0^2}{n}) \)</p>

  <p><strong>Example:</strong> Coffee volume \( \sim \mathcal{N}(8, 0.47) \), sample size \( n = 10 \):</p>
  <ul>
    <li>\( \bar{X} \sim \mathcal{N}(8, \frac{0.47}{10}) \)</li>
    <li>\( P(\bar{X} > 8.5) = P(Z > 2.306) \approx 0.0105 \)</li>
  </ul>

  <p><strong>Moment Generating Function:</strong> \( M(t) = \mathbb{E}[e^{tX}] \)</p>
  <p>\( \frac{d^n}{dt^n} M(t) \bigg|_{t = 0} = \mathbb{E}[X^n] \)</p>
  <hr>

</div>
</body>
</html>
