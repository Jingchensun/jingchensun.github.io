<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Chapter 6: Estimation - Maximum Likelihood</title>
  <link rel="stylesheet" href="../../stylesheet.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    h1 {
      text-align: center;
      color: #527bbd;
      font-size: 165%;
      margin-bottom: 40px;
      border-bottom: none;
    }

    h2 {
      color: #2471A3;
      font-size: 125%;
      margin-top: 2em;
    }

    p, ul, li {
      font-size: 16px;
      line-height: 1.6;
    }

    hr {
      border: none;
      border-top: 1px solid #ccc;
      margin: 40px 0;
    }

    code {
      background-color: #f8f8f8;
      padding: 2px 4px;
      border-radius: 3px;
    }

    pre {
      background-color: #f4f4f4;
      padding: 10px;
      border-left: 4px solid #ccc;
      overflow-x: auto;
    }
  </style>
</head>

<body>
<div id="layout-content" style="max-width: 960px; margin: 30px auto; padding: 0 50px;">

  <h1>Chapter 6: Estimation</h1>

  <h2>Section 6.1: Maximum Likelihood Estimation (MLE)</h2>
  <p><strong>Example: Coin Flip</strong></p>
  <p>Observation: H, H, H, T</p>
  <p>Let \( p \) be the probability of heads. Then:</p>
  <p>\[
    P(HHHT) = p^3 (1 - p)
  \]</p>
  <p>Define the likelihood function:</p>
  <p>\[
    L(p) = p^3 (1 - p)
  \]</p>
  <p>To find the MLE of \( p \), solve for \( L'(p) = 0 \):</p>
  <p>\[
    L'(p) = 3p^2 - 4p^3 = p^2 (3 - 4p)
  \]</p>
  <p>Solutions: \( p = 0 \) or \( p = \frac{3}{4} \). Since \( L(0) = 0 \), the MLE is:</p>
  <p>\[
    \hat{p}_{\text{MLE}} = \frac{3}{4}
  \]</p>

  <hr>

  <h2>MLE for Discrete Distributions</h2>
  <p>Let \( X_1, ..., X_n \) be i.i.d. samples from a discrete distribution with PMF \( f(x; \theta) \).</p>
  <p>The likelihood function is:</p>
  <p>\[
    L(\theta) = \prod_{i=1}^n f(x_i; \theta)
  \]</p>

  <h3>Example: Poisson Distribution</h3>
  <p>PMF: \( f(x; \lambda) = \frac{\lambda^x e^{-\lambda}}{x!} \)</p>
  <p>Observed data: \( x_1 = 3, x_2 = 4, x_3 = 3, x_4 = 7 \)</p>
  <p>\[
    L(\lambda) = \frac{\lambda^{17} e^{-4\lambda}}{3! \cdot 4! \cdot 3! \cdot 7!}
  \]</p>
  <p>Log-likelihood:</p>
  <p>\[
    \ln L(\lambda) = 17 \ln \lambda - 4\lambda - \ln(\#)
  \]</p>
  <p>Differentiating and solving:</p>
  <p>\[
    \frac{17}{\lambda} - 4 = 0 \Rightarrow \hat{\lambda}_{\text{MLE}} = \frac{17}{4}
  \]</p>

  <hr>

  <h2>MLE for Continuous Distributions</h2>
  <p>Let \( X_1, ..., X_n \) be i.i.d. samples from a continuous distribution with PDF \( f(x; \theta) \).</p>
  <p>\[
    L(\theta) = \prod_{i=1}^n f(x_i; \theta)
  \]</p>

  <h3>Example: Exponential Distribution</h3>
  <p>PDF: \( f(x; \lambda) = \lambda e^{-\lambda x}, \quad x \ge 0 \)</p>
  <p>\[
    L(\lambda) = \lambda^n e^{-\lambda \sum x_i}
  \]</p>
  <p>Log-likelihood:</p>
  <p>\[
    \ln L(\lambda) = n \ln \lambda - \lambda \sum x_i
  \]</p>
  <p>Solving:</p>
  <p>\[
    \frac{n}{\lambda} - \sum x_i = 0 \Rightarrow \hat{\lambda}_{\text{MLE}} = \frac{n}{\sum x_i} = \frac{1}{\bar{x}}
  \]</p>

  <hr>

  <h2>MLE for Cauchy Distribution (Numerical Optimization)</h2>
  <p>Distribution: \( X \sim \text{Cauchy}(\theta) \)</p>
  <p>PDF: \( f(x; \theta) = \frac{1}{\pi(1 + (x - \theta)^2)} \)</p>
  <p>Data: \( x = [1, 2, 3] \)</p>
  <p>Log-likelihood:</p>
  <p>\[
    \ln L(\theta) = -3 \ln \pi - \sum_{i=1}^3 \ln(1 + (x_i - \theta)^2)
  \]</p>
  <p>Score equation:</p>
  <p>\[
    \sum_{i=1}^3 \frac{2(x_i - \theta)}{1 + (x_i - \theta)^2} = 0
  \]</p>
  <p>This is nonlinear in \( \theta \), so solve numerically.</p>

  <h3>R Code for Optimization:</h3>
  <pre><code>
x <- c(1, 2, 3)
logL <- function(theta) sum(log(dcauchy(x, theta)))
optimize(logL, interval = c(0, 4), maximum = TRUE)
  </code></pre>
  <p><strong>Output:</strong> maximum = 2, objective = -4.8205</p>

  <hr>

  <h2>MLE with Two Parameters in Normal Distribution</h2>
  <p>Distribution: \( X \sim N(\mu, \sigma^2) \)</p>
  <p>PDF: \( f(x; \mu, \sigma) = \frac{1}{\sqrt{2\pi} \sigma} \exp\left( -\frac{(x - \mu)^2}{2\sigma^2} \right) \)</p>

  <p>Log-likelihood:</p>
  <p>\[
    \ln L(\mu, \sigma) = -\frac{n}{2} \ln(2\pi) - n \ln \sigma - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2
  \]</p>

  <p><strong>MLE for \( \mu \):</strong></p>
  <p>\[
    \frac{\partial}{\partial \mu} \ln L = 0 \Rightarrow \mu = \frac{1}{n} \sum x_i = \bar{x}
  \]</p>

  <p><strong>MLE for \( \sigma \):</strong></p>
  <p>\[
    \frac{\partial}{\partial \sigma} \ln L = 0 \Rightarrow \sigma^2 = \frac{1}{n} \sum (x_i - \bar{x})^2
  \]</p>

  <p><strong>Final MLE Estimates:</strong></p>
  <p>\[
    \hat{\mu}_{\text{MLE}} = \bar{x}, \quad 
    \hat{\sigma}_{\text{MLE}} = \sqrt{ \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2 }
  \]</p>

  <hr>

  <h2>Section 6.2: Method of Moments (MOM)</h2>

  <p>Let \( X \) be a random variable with PDF \( f(x; \theta) \), and observed sample \( x_1, x_2, \dots, x_n \).</p>

  <p>The <strong>r-th moment</strong> of the distribution is:</p>
  <p>\[
  \mathbb{E}[X^r] = \int_{-\infty}^{\infty} x^r f(x; \theta) dx
  \]</p>
  <p>The corresponding <strong>sample moment</strong> is:</p>
  <p>\[
  \frac{1}{n} \sum_{i=1}^n x_i^r
  \]</p>

  <p><strong>Key idea:</strong> Match theoretical moments with sample moments to solve for parameters.</p>

  <h3>Case 1: Single parameter \( \theta \)</h3>
  <p>Match the first moment:</p>
  <p>\[
  \mathbb{E}[X] = \int x f(x; \theta) dx = \frac{1}{n} \sum_{i=1}^n x_i = \bar{x}
  \Rightarrow \hat{\theta}_{\text{MOM}} = \text{solution to the above}
  \]</p>

  <h3>Example: Exponential Distribution</h3>
  <p>\( f(x; \lambda) = \lambda e^{-\lambda x}, \quad x \ge 0, \lambda > 0 \)</p>
  <p>Theoretical mean:</p>
  <p>\[
  \mathbb{E}[X] = \int_0^\infty x \lambda e^{-\lambda x} dx = \frac{1}{\lambda}
  \]</p>
  <p>Set equal to sample mean:</p>
  <p>\[
  \frac{1}{\lambda} = \bar{x} \Rightarrow \hat{\lambda}_{\text{MOM}} = \frac{1}{\bar{x}}
  \]</p>

  <h3>Case 2: Two parameters (e.g., Normal Distribution)</h3>
  <p>\( X \sim N(\mu, \sigma^2) \), match moments for \( r = 1, 2 \):</p>

  <ul>
    <li>First moment: \(\mathbb{E}[X] = \mu \Rightarrow \hat{\mu}_{\text{MOM}} = \bar{x}\)</li>
    <li>Second moment: \(\mathbb{E}[X^2] = \sigma^2 + \mu^2\)</li>
  </ul>

  <p>Sample second moment:</p>
  <p>\[
  \frac{1}{n} \sum_{i=1}^n x_i^2 = \mathbb{E}[X^2] \Rightarrow \hat{\sigma}_{\text{MOM}}^2 = \frac{1}{n} \sum x_i^2 - \bar{x}^2
  \]</p>

  <p>This is equivalent to the non-adjusted sample variance (denominator is \( n \), not \( n - 1 \)).</p>
  <hr>

  <h2>Section 6.3: Properties of Estimators</h2>

  <p>Let \( \hat{\theta} \) be an estimator of parameter \( \theta \) from a sample.</p>
  <p>The <strong>bias</strong> of \( \hat{\theta} \) is defined as:</p>
  <p>\[
  \text{Bias}[\hat{\theta}] = \mathbb{E}[\hat{\theta}] - \theta
  \]</p>
  <p>Ideally, we want the bias to be zero (i.e., the estimator is unbiased).</p>

  <h3>Example: Estimating the Mean</h3>
  <p>Suppose we use the sample mean to estimate \( \mu = \mathbb{E}[X] \).</p>
  <p>Let \( \hat{\mu} = \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i \). Then:</p>
  <p>\[
  \text{Bias}[\bar{X}] = \mathbb{E}[\bar{X}] - \mu
  = \mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n X_i\right] - \mu
  = \frac{1}{n} \sum_{i=1}^n \mathbb{E}[X_i] - \mu = \mu - \mu = 0
  \]</p>

  <p><strong>Conclusion:</strong> The sample mean \( \bar{X} \) is an unbiased estimator of \( \mu \).</p>
  <p>This applies whether \( \bar{X} \) is derived via MLE or Method of Moments.</p>

  <p><em>See handout "Ch 6 bias" for more on variance and MSE of estimators.</em></p>

  <hr>

  <h2>Section 6.3: Properties of Estimators (Continued)</h2>

  <h3>Unbiased Estimator of Variance</h3>
  <p><strong>Theorem 6.3.2:</strong> Let \( X_1, \dots, X_n \) be independent random variables with variance \( \text{Var}(X_i) = \sigma^2 \). Then an unbiased estimator of \( \sigma^2 \) is:</p>
  <p>\[
  S^2 = \frac{1}{n - 1} \sum_{i=1}^n (X_i - \bar{X})^2
  \]</p>
  <p>This is the sample variance. The factor \( \frac{1}{n - 1} \) is used to make it unbiased.</p>

  <h3>Why Divide by \( n-1 \)?</h3>
  <p>If we instead use:</p>
  <p>\[
  \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2
  \]</p>
  <p>Then the bias is:</p>
  <p>\[
  \text{Bias}[\hat{\sigma}^2] = \mathbb{E}[\hat{\sigma}^2] - \sigma^2 = -\frac{\sigma^2}{n} \ne 0
  \]</p>

  <p>We correct this by rescaling:</p>
  <p>\[
  S^2 = \frac{n}{n - 1} \hat{\sigma}^2 \Rightarrow \mathbb{E}[S^2] = \sigma^2
  \]</p>

  <h3>Asymptotic Bias</h3>
  <p>An estimator is <strong>asymptotically unbiased</strong> if:</p>
  <p>\[
  \lim_{n \to \infty} \text{Bias}[\hat{\theta}] = 0
  \]</p>
  <p>For example:</p>
  <p>\[
  \lim_{n \to \infty} \text{Bias}[\hat{\sigma}^2] = 0
  \Rightarrow \hat{\sigma}^2 \text{ is asymptotically unbiased}
  \]</p>

  <hr>

  <h3>Efficiency of Estimators</h3>
  <p><strong>Efficiency</strong> measures how "tight" the estimator is. Given two unbiased estimators \( \hat{\theta}_1, \hat{\theta}_2 \):</p>
  <p>\[
  \text{If } \text{Var}[\hat{\theta}_1] < \text{Var}[\hat{\theta}_2] \Rightarrow \hat{\theta}_1 \text{ is more efficient}
  \]</p>

  <p><strong>Example:</strong></p>
  <ul>
    <li>\( \hat{\mu}_1 = \frac{1}{3}(X_1 + X_2 + X_3) \Rightarrow \text{Var} = \frac{\sigma^2}{3} \)</li>
    <li>\( \hat{\mu}_2 = \frac{1}{6}X_1 + \frac{1}{3}X_2 + \frac{1}{2}X_3 \Rightarrow \text{Var} = \frac{7}{18} \sigma^2 \)</li>
  </ul>
  <p>Conclusion: \( \hat{\mu}_1 \) is more efficient.</p>

  <hr>

  <h3>Mean Squared Error (MSE)</h3>
  <p>The mean squared error is:</p>
  <p>\[
  \text{MSE}[\hat{\theta}] = \mathbb{E}[(\hat{\theta} - \theta)^2] = \text{Var}[\hat{\theta}] + (\text{Bias}[\hat{\theta}])^2
  \]</p>

  <p>MSE combines both variance and bias, and is a useful metric when estimators are biased.</p>

  <p><strong>Example: \( X \sim \text{Binomial}(n, p) \)</strong></p>
  <ul>
    <li>Estimator 1: \( \hat{p}_1 = \frac{X}{n} \)</li>
    <li>\( \text{Bias}[\hat{p}_1] = 0, \quad \text{MSE} = \frac{p(1 - p)}{n} \)</li>
    <li>Estimator 2: \( \hat{p}_2 = \frac{X + 1}{n + 2} \)</li>
    <li>\( \text{Bias}[\hat{p}_2] \ne 0, \quad \text{MSE} = \frac{np(1 - p)}{(n + 2)^2} + \left( \frac{1 - 2p}{n + 2} \right)^2 \)</li>
  </ul>

  <p>Depending on values of \( n \) and \( p \), either estimator may have lower MSE.</p>

  <hr>

  <h3>Consistency of Estimators</h3>
  <p>An estimator \( \hat{\theta} \) is <strong>consistent</strong> if:</p>
  <p>\[
  \lim_{n \to \infty} P(|\hat{\theta} - \theta| < \varepsilon) = 1 \quad \text{for any } \varepsilon > 0
  \]</p>

  <p><strong>Example:</strong> Let \( X_1, ..., X_n \sim N(\mu, 1) \), then:</p>
  <p>\[
  \bar{X}_n = \frac{1}{n} \sum X_i \quad \Rightarrow \mathbb{E}[\bar{X}_n] = \mu, \quad \text{Var}[\bar{X}_n] = \frac{1}{n}
  \]</p>
  <p>Since variance goes to 0 as \( n \to \infty \), \( \bar{X}_n \) is a consistent estimator of \( \mu \).</p>

  <p><strong>Alternative criterion:</strong></p>
  <p>If \( \text{MSE}[\hat{\theta}_n] \to 0 \) as \( n \to \infty \), then \( \hat{\theta}_n \) is consistent.</p>

</div>
</body>
</html>
