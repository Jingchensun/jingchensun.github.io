<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Chapter 9: Regression and Correlation (Full Notes)</title>
  <link rel="stylesheet" href="../../stylesheet.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    body {
      font-family: Georgia, serif;
      background: #fff;
      color: #111;
      line-height: 1.6;
      padding: 2em;
      max-width: 960px;
      margin: auto;
    }
    h1 {
      text-align: center;
      color: #527bbd;
      font-size: 165%;
    }
    h2 {
      color: #2471A3;
      margin-top: 2em;
    }
    p, ul, li {
      font-size: 16px;
    }
    code {
      background: #f4f4f4;
      padding: 2px 4px;
      border-radius: 4px;
    }
    hr {
      margin: 3em 0;
    }
  </style>
</head>
<body>

<h1>Chapter 9: Regression and Correlation</h1>

<h2>Section 9.1 Covariance</h2>
<p>\(\text{Cov}(X, Y) = \mathbb{E}[(X - \mu_X)(Y - \mu_Y)] = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]\)</p>
<p>\(X, Y\) independent if \(\text{Cov}(X, Y) = 0\)</p>
<p>Positively correlated if \(> 0\), negatively if \(< 0\)</p>
<p>\(\text{Cov}(X, X) = \text{Var}(X) \ge 0\)</p>

<h3>Covariance of a Sum</h3>
<p>\(\text{Cov}(X_1 + X_2, Y) = \text{Cov}(X_1, Y) + \text{Cov}(X_2, Y)\)</p>

<h3>Variance of a Sum</h3>
<p>\(\text{Var}(X + Y) = \text{Var}(X) + 2\text{Cov}(X, Y) + \text{Var}(Y)\)</p>
<p>If \(X, Y\) independent: \(\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)\)</p>

<h2>Joint PDF Covariance Example</h2>
<p>Given \(f(x, y) = \frac{3}{2}(x^2 + y^2)\), for \(0 < x < 1, 0 < y < 1\), compute:</p>
<p>\(\mathbb{E}[XY] = \frac{3}{8}\), \(\mathbb{E}[X] = \mathbb{E}[Y] = \frac{5}{8}\)</p>
<p>\(\text{Cov}(X, Y) = \frac{3}{8} - \frac{25}{64} = -\frac{1}{64}\)</p>

<hr>

<h2>Section 9.2 Correlation</h2>
<p>\(\rho(X, Y) = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X)} \cdot \sqrt{\text{Var}(Y)}}\)</p>

<p>Linear transformation property: \(\rho(a + bX, Y) = \rho(X, Y)\)</p>

<p>Let \(X = \mu_X + \sigma_X Z_X, Y = \mu_Y + \sigma_Y Z_Y\), then \(\rho(X, Y) = \rho(Z_X, Z_Y)\)</p>

<p>Proof that \(|\rho| \le 1\): via \(\text{Var}(Z_X \pm Z_Y)\)</p>
<p>\(\rho = \pm 1 \Rightarrow Y = \frac{\sigma_Y}{\sigma_X}X + a\)</p>

<hr>

<h2>Sample Correlation</h2>
<p>\(r \approx \rho = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2} \cdot \sqrt{\sum (y_i - \bar{y})^2}}\)</p>
<p>Alternate form:</p>
<p>\(r = \frac{\frac{1}{n} \sum x_i y_i - \bar{x} \bar{y}}{s_x s_y}\)</p>

<p>Interpretation:</p>
<ul>
  <li>\(r = -1\): perfect negative line</li>
  <li>\(r = 0\): no correlation</li>
  <li>\(r = +1\): perfect positive line</li>
</ul>

<hr>

<h2>Section 9.3 Least Squares Regression</h2>
<p>Best fit line: \(y = a + bx\)</p>
<p>Residuals: \(\varepsilon_i = y_i - (a + b x_i)\)</p>
<p>Minimize \(S(a,b) = \sum \varepsilon_i^2 = \sum [y_i - (a + bx_i)]^2\)</p>

<h3>Normal Equations</h3>
<p>Solve for \(a, b\):</p>
<p>\(b = \frac{\overline{xy} - \bar{x} \bar{y}}{\overline{x^2} - \bar{x}^2}\)</p>
<p>\(a = \bar{y} - b \bar{x}\)</p>

<p>Alternate form: \(b = \frac{\text{Cov}(x, y)}{\text{Var}(x)}\)</p>

<hr>

<h2>Properties of Regression Lines</h2>
<p>From correlation: \(b = r \cdot \frac{s_y}{s_x}\)</p>

<h3>\(r^2\) as Goodness of Fit</h3>
<p>\(r^2 = \frac{\text{variation in predicted}}{\text{variation in observed}} = \frac{\sum (\hat{y}_i - \bar{y})^2}{\sum (y_i - \bar{y})^2}\)</p>
<p>Proof: derive from model \(\hat{y}_i = a + bx_i\)</p>
<p>\(0 \le r^2 \le 1\), measures proportion of variation explained by the model</p>
<p><em>See handout <a href="ch9_regression.html" target="_blank">ch9_regression.html</a>:
<hr>

<h2>Residual Analysis</h2>
<p>Residuals: \(\varepsilon_i = y_i - \hat{y}_i\)</p>
<p>Residual variance: \(s_e^2 = \frac{1}{n - 1} \sum (y_i - \hat{y}_i)^2\)</p>

<p><strong>Plot of residuals vs x:</strong></p>
<ul>
  <li>Should be random with no trend</li>
  <li>Parabolic pattern â‡’ use nonlinear regression</li>
</ul>

<p><strong>Outliers:</strong> Be cautious; only remove with justification</p>

<hr>

<h2>Section 9.4 Inference on Parameters</h2>
<p>Use inference for \(a, b\): standard errors, confidence intervals</p>

<h2>Other Regression Models</h2>
<ul>
  <li>Multiple regression: \(\hat{y} = a + b x_1 + c x_2 + d x_3\)</li>
  <li>Higher-order regression: \(\hat{y} = a + bx + cx^2 + dx^3\)</li>
</ul>

</body>
</html>