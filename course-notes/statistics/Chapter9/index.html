<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Chapter 9: Regression and Correlation</title>
  <link rel="stylesheet" href="../../stylesheet.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    h1 {
      text-align: center;
      color: #527bbd;
      font-size: 165%;
      margin-bottom: 40px;
      border-bottom: none;
    }

    h2 {
      color: #2471A3;
      font-size: 125%;
      margin-top: 2em;
    }

    p, ul, li {
      font-size: 16px;
      line-height: 1.6;
    }

    hr {
      border: none;
      border-top: 1px solid #ccc;
      margin: 40px 0;
    }
  </style>
</head>

<body>
<div id="layout-content" style="max-width: 960px; margin: 30px auto; padding: 0 50px;">

  <h1>Chapter 9: Regression and Correlation</h1>

  <h2>Section 9.1: Covariance</h2>
  <p>\(\text{Cov}(X, Y) = \mathbb{E}[(X - \mu_X)(Y - \mu_Y)] = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]\)</p>
  <ul>
    <li>If \( X, Y \) are independent, then \( \text{Cov}(X, Y) = 0 \)</li>
    <li>Positive correlation: \( \text{Cov}(X, Y) > 0 \)</li>
    <li>Negative correlation: \( \text{Cov}(X, Y) < 0 \)</li>
    <li>\( \text{Cov}(X, X) = \text{Var}(X) \geq 0 \)</li>
  </ul>

  <p><strong>Covariance of a Sum:</strong></p>
  <p>\(\text{Cov}(X_1 + X_2, Y) = \text{Cov}(X_1, Y) + \text{Cov}(X_2, Y)\)</p>

  <p><strong>Variance of a Sum:</strong></p>
  <p>\(\text{Var}(X + Y) = \text{Var}(X) + 2\text{Cov}(X, Y) + \text{Var}(Y)\)</p>
  <p>If \( X \perp Y \), then: \(\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)\)</p>

  <h2>Calculating Covariance from Joint PDF</h2>
  <p>Example joint pdf: \( f(x, y) = \frac{3}{2}(x^2 + y^2), \quad 0 < x, y < 1 \)</p>
  <p>Then: \(\text{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y] = \frac{3}{8} - \frac{5}{8} \cdot \frac{5}{8} = -\frac{1}{64}\)</p>

  <hr>

  <h2>Section 9.2: Correlation</h2>
  <p>\(\rho(X, Y) = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X)} \cdot \sqrt{\text{Var}(Y)}}\)</p>
  <p>\(\rho\) is invariant to linear transformation: \(\rho(a + bX, Y) = \rho(X, Y)\)</p>

  <p>Let \( X = \mu_X + \sigma_X Z_X, \quad Y = \mu_Y + \sigma_Y Z_Y \), then:</p>
  <p>\(\rho(X, Y) = \rho(Z_X, Z_Y) = \text{Cov}(Z_X, Z_Y)\)</p>

  <p>\( |\rho| \leq 1 \), with equality if \( Y = \frac{\sigma_Y}{\sigma_X}X + a \)</p>

  <hr>

  <h2>Sample Correlation</h2>
  <p>\( r \approx \rho = \frac{1}{n} \sum (x_i - \bar{x})(y_i - \bar{y}) \bigg/ \sqrt{\frac{1}{n} \sum (x_i - \bar{x})^2} \cdot \sqrt{\frac{1}{n} \sum (y_i - \bar{y})^2} \)</p>

  <p>Alternate form:</p>
  <p>\( r = \frac{ \left( \frac{1}{n} \sum x_i y_i \right) - \bar{x}\bar{y} }{ s_x s_y } \)</p>

  <p><strong>Interpretation of \( r \):</strong></p>
  <ul>
    <li>\( r = -1 \): perfect negative linear relationship</li>
    <li>\( r = 0 \): no correlation</li>
    <li>\( r = +1 \): perfect positive linear relationship</li>
  </ul>

  <hr>

  <h2>Section 9.3: Least Squares Regression</h2>
  <p>Model: \( \hat{y} = a + bx \)</p>
  <p>Minimize squared error: \( S(a, b) = \sum (y_i - (a + b x_i))^2 \)</p>

  <p>Solution:</p>
  <ul>
    <li>\( b = \frac{\overline{xy} - \bar{x} \bar{y}}{\overline{x^2} - (\bar{x})^2} \)</li>
    <li>\( a = \bar{y} - b\bar{x} \)</li>
  </ul>

  <p>Alternate form for slope:</p>
  <p>\( b = \frac{ \sum (x_i - \bar{x})(y_i - \bar{y}) }{ \sum (x_i - \bar{x})^2 } = \frac{\text{Cov}(x, y)}{\text{Var}(x)} \)</p>

  <hr>

  <h2>Properties of Regression Lines</h2>
  <p>\( b = r \cdot \frac{s_y}{s_x} \)</p>
  <p>\( r^2 = \frac{\text{variation in predicted values}}{\text{variation in observed values}} \)</p>
  <p>\( r^2 \in [0, 1] \): indicates how well the regression explains the variation in \( y \)</p>
  <p><em>See R code<a href="ch9_regression.html" target="_blank">ch9_regression.html</a> for full derivations and examples.</em></p>
  
  <hr>

  <h2>Residual Analysis</h2>
  <p>Residual: \( \varepsilon_i = y_i - \hat{y}_i \)</p>
  <p>Variance of residuals: \( s_e^2 = \frac{1}{n - 1} \sum (y_i - \hat{y}_i)^2 \)</p>

  <p><strong>Plot of residuals vs x</strong> should show random scatter without trend.</p>
  <p>If residuals show a pattern (e.g., parabolic), consider nonlinear models.</p>

  <p><strong>Outliers:</strong> Can affect small datasets; only remove if justified.</p>

  <p><strong>Section 9.4:</strong> Inference on regression coefficients \( a, b \), including error bars.</p>

  <h2>Other Regression Models</h2>
  <ul>
    <li><strong>Multiple regression:</strong> \( \hat{y} = a + b x_1 + c x_2 + d x_3 \)</li>
    <li><strong>Higher-order regression:</strong> \( \hat{y} = a + b x + c x^2 + d x^3 \)</li>
  </ul>

</div>
</body>
</html>
