<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="keywords" content="Jingchen Sun, UB, State University of New York at Buffalo, ZJU, Zhejiang University, Â≠ôÁ≤æËæ∞, Á∫ΩÁ∫¶Â∑ûÁ´ãÂ§ßÂ≠¶Â∏ÉÊ≥ïÁΩóÂàÜÊ†°, Â∏ÉÊ≥ïÁΩóÂ§ßÂ≠¶, ÊµôÊ±üÂ§ßÂ≠¶, CS, CSE, Computer Science, Computer Science and Engineering, ËÆ°ÁÆóÊú∫Á≥ª">
  <meta name="description" content="Jingchen Sun&#39;s Homepage">
  <link rel="stylesheet" href="./stylesheet.css" type="text/css">
  <style type="text/css">
      h2 {
        color: #2471A3; /* ËìùËâ≤Â≠ó‰Ωì */
      }
  
      /* Â¶ÇÊûú‰Ω†ÊÉ≥Ë¶ÅÊõ¥Âä†ÈÄöÁî®Âú∞‰øÆÊîπÊâÄÊúâÊ†áÈ¢ò */
      h2, h3 {
        color: #2471A3; /* ËìùËâ≤Â≠ó‰Ωì */
      }
  
      .update {
        color: red;
      }

      ul {
          list-style-type: none; /* Remove the bullet points */
          padding-left: 0; /* Remove default padding */
      }

      li {
          list-style-type: none; /* Remove bullet points for each list item */
      }

      /* Button styles for DOI, PDF, BibTeX, Project Webpage */
      a.btn {
          background-color: #2471a3; 
          color: #fff; 
          font-size: 12px; 
          margin-top: 4px; 
          display: inline-block; 
          padding: 0 4px; 
          border-radius: 2px; 
          line-height: 1.5em;
      }

      /* Hover effect for general buttons */
      a.btn:hover {background-color: #174e74;}

      /* Specific styles for each button type */
      a.btn-orange {background-color: #e2700c;}
      a.btn-orange:hover {background-color: #c46009;}
      
      a.btn-red {background-color: #d63a3a;}
      a.btn-red:hover {background-color: #a02727;}
      
      a.btn-blue {background-color: #2471a3;}
      a.btn-blue:hover {background-color: #174e74;}

      /* Image size adjustment */
      img.publication-image {
        width: 100%; /* Make the image fill the 40% width of the table cell */
        height: auto; /* Maintain aspect ratio */
    }
  </style>
  <title>Jingchen Sun - Homepage</title>
</head>

<body>

<!-- üîº Ê∑ªÂä†ÂØºËà™Ê†èÂú®ËøôÈáå -->
<div style="text-align: right; padding: 50px 30px 0 0;">
    <a href="./course-notes/index.html" style="font-size: 18px; font-weight: bold; color: #1a5276;">My Machine Learning Notes</a>
  </div>
  

<div id="layout-content" style="margin-top:25px">

  <table>
  <tbody>
    <tr>
      <td width="39%" valign="top" height="193">
        <img height="225" id="photo" style="padding: 0pt 30pt 0pt 20pt; float: left; display: inline;" src="./jingchensun.jpeg">
      </td>
      
      <td width="60%" valign="top" height="193">
        <b><font face="Times New Roman" size="6">Jingchen Sun</font><font size="6" face="Ê•∑‰Ωì_GB2312"> ÔºàÂ≠ôÁ≤æËæ∞Ôºâ</font><font face="Times New Roman" size="6"></font></b>
        <br><br>    
        <p>          
            Ph.D. Student
        </p>  
        <p>          
          <a href="https://engineering.buffalo.edu/computer-science-engineering.html" target="_blank"> Department of Computer Science and Engineering</a>
        </p>  
        <p>          
            University at Buffalo, State University of New York
        </p>
        <p>          
            Email: jsun39@buffalo.edu
        </p>
        <p>
          <a href="https://github.com/Jingchensun" target="_blank">
            <img src="logo/logo-github.png" alt="Github" style="width: 30px; height: 30px; margin-right: 20px;">
          </a>
         
          <a href="https://scholar.google.com/citations?hl=en&user=7Ow7PQkAAAAJ" target="_blank">
            <img src="logo/logo-googlescholar.png" alt="Google Scholar" style="width: 30px; height: 30px; margin-right: 20px;">
          </a>
        
          <a href="https://www.linkedin.com/in/jingchensun/" target="_blank">
            <img src="logo/logo-linkin.png" alt="LinkedIn" style="width: 30px; height: 30px;">
          </a>
        </p>
        
      </td>
    </tr>
  </tbody>
  </table>

  <h2>About Me</h2> <!-- Â∞ÜÂ≠ó‰ΩìÈ¢úËâ≤ËÆæÁΩÆ‰∏∫ËìùËâ≤ -->
  <p style="text-align:justify;">
    I am a Ph.D. student in Computer Science at <a href="https://www.buffalo.edu/" target="_blank">University at Buffalo, State University of New York</a>, fortunate to be supervised by Prof. <a href="https://cse.buffalo.edu/~changyou/index.html" target="_blank">Changyou Chen</a>. 
    Prior to this, I obtained my Master's degree from Zhejiang University and my Bachelor's degree from North China Electric Power University.
  </p>
   
  <!-- <p style="text-align:justify;"> -->
    <!-- My research primarily focuses on multi-modal model representation learning and transfer learning. This includes, but is not limited to, vision-language models (e.g., <a href="https://github.com/openai/CLIP" target="_blank">CLIP</a>), audio-language models (e.g., <a href="https://github.com/microsoft/CLAP" target="_blank">CLAP</a>), and speech-language models (e.g., <a href="https://github.com/openai/whisper" target="_blank">Whisper</a>). -->
    <!-- I have developed several parameter and compute-efficient methods, such as cross-modal prompt tuning and training-free support sets, to facilitate and enhance the application of these multi-modal models in downstream tasks. -->
    <p style="text-align:justify;">
        My research primarily focuses on <strong>Multi-modal Large Language Lodels (LLMs)</strong>, including but not limited to <em>vision-language</em> and <em>audio-language</em> models. I have developed several parameter-efficient methods, such as <em>cross-modal prompt tuning</em> and <em>training-free support sets</em>, to facilitate and enhance the application of these pretrained LLMs in downstream tasks.    
      </p>
      
      <p style="text-align:justify;">
        I am currently interested in leveraging Retrieval-Augmented Generation (RAG) and Knowledge Distillation (KD) to enhance the reasoning capabilities of multimodal LLMs. If you are also interested in related topics and would like to collaborate, feel free to drop me an email!
      </p>

  <h2>News <span class="update" style="font-size: 0.7em;">[Update!]</span></h2>
  <ul>
    <li>
        02/21/2026: One paper were accepted to CVPR 2026!
  </li>
    <li>
        01/17/2026: One paper were accepted to ICASSP 2026!
  </li>
    <li>
        11/07/2025: One paper were accepted to AAAI 2026!
  </li>
    <li>
        12/20/2024: Two papers were accepted to ICASSP 2025!
  </li>
    <li>
        10/28/2024: A paper was accepted to WACV 2025!
    </li>
    <li>
        09/25/2024: A paper was accepted to NeurIPS 2024!
    </li>
      <li>
        08/23/2024: I finished the intern at NEC Labs, great summer at Princeton !
    </li>
    <!-- <li>
        03/21/2023: I will join in Nokia-Bell Labs as an applied AI researcher intern in summer 2023!
    </li> -->
  </ul>

  <h2>Highlight Publications</h2>
  <ul>
    <li>
        <table style="width:100%; margin-bottom: 20px;">
            <tr>
                <!-- Image on the left with larger size -->
                <td style="width:40%; vertical-align: top;">
                    <img src="publication/beta-kd.png" alt="Craft Image" class="publication-image">
                </td>
                
                <!-- Text on the right -->
                <td style="width:60%; vertical-align: top;">
                    <b style="font-size: 18px; color: #1A5276;">Beta-KD: Uncertainty-Aware Knowledge Distillation for Multimodal Large Language
                        Models</b><br>
                        <b>Jingchen Sun</b>, Shaobo Han,  Deep Anil Patel, Can Jin, Changyou Chen. <br>
                        <br>We propose a novel uncertainty-aware knowledge distillation method, which can improve the performance of the student model by leveraging the uncertainty of the teacher model.
                    <span style="font-style: italic;">CVPR 2026</span><br>
  
                    <!-- Links for DOI, PDF, BibTex, Project Webpage -->
                    <a href="https://jingchensun.github.io/" target="_blank" class="btn btn-orange">Code</a>
                    <a href="https://jingchensun.github.io/" target="_blank" class="btn btn-red">PDF</a>
                </td>
            </tr>
        </table>
    </li>

    <li>
        <table style="width:100%; margin-bottom: 20px;">
            <tr>
                <!-- Image on the left with larger size -->
                <td style="width:40%; vertical-align: top;">
                    <img src="publication/clap-support.png" alt="Craft Image" class="publication-image">
                </td>
                
                <!-- Text on the right -->
                <td style="width:60%; vertical-align: top;">
                    <b style="font-size: 18px; color: #1A5276;">CLAP-S: Support Set-Based Adaption for Downstream Fiber-Optic Acoustic Recognition</b><br>
                    <b>Jingchen Sun</b>, Shaobo Han, Wataru Kohno, Changyou Chen. <br>
                    <span style="font-style: italic;">ICASSP 2025</span><br>
  
                    <!-- Links for DOI, PDF, BibTex, Project Webpage -->
                    <a href="https://github.com/Jingchensun/clap-s" target="_blank" class="btn btn-orange">Code</a>
                    <a href="https://arxiv.org/abs/2501.09877" target="_blank" class="btn btn-red">PDF</a>
                </td>
            </tr>
        </table>
    </li>

    <li>
      <table style="width:100%; margin-bottom: 20px;">
          <tr>
              <!-- Image on the left with larger size -->
              <td style="width:40%; vertical-align: top;">
                  <img src="publication/craft.png" alt="Craft Image" class="publication-image">
              </td>
              
              <!-- Text on the right -->
              <td style="width:60%; vertical-align: top;">
                  <b style="font-size: 18px; color: #1A5276;">Craft: Cross-modal Aligned Features Improve Robustness of Prompt Tuning</b><br>
                  <b>Jingchen Sun</b>,  Rohan Sharma, Vishnu Suresh Lokhande, Changyou Chen<br>
                  <span style="font-style: italic;">WACV 2025</span><br>

                  <!-- Links for DOI, PDF, BibTex, Project Webpage -->
                  <a href="https://github.com/Jingchensun/Craft" target="_blank" class="btn btn-orange">Code</a>
                  <a href="https://arxiv.org/pdf/2407.15894" target="_blank" class="btn btn-red">PDF</a>
              </td>
          </tr>
      </table>
  </li>

    <li>
        <table style="width:100%; margin-bottom: 20px;">
            <tr>
                <!-- Image on the left with larger size -->
                <td style="width:40%; vertical-align: top;">
                    <img src="publication/neruips.png" alt="Craft Image" class="publication-image">
                </td>
                
                <!-- Text on the right -->
                <td style="width:60%; vertical-align: top;">
                    <b style="font-size: 18px; color: #1A5276;">A probability contrastive learning framework for 3D molecular representation learning</b><br>
                    Jiayu Qin, Jian Chen, Rohan Sharma, <b>Jingchen Sun</b>, Changyou Chen<br>
                    <span style="font-style: italic;">NeurIPS 2024</span><br>

                    <!-- Links for DOI, PDF, BibTex, Project Webpage -->
                    <a href="papers/neurips.pdf" target="_blank" class="btn btn-orange">Code</a>
                    <a href="papers/neurips.pdf" target="_blank" class="btn btn-red">PDF</a>
                </td>
            </tr>
        </table>
    </li>

    <li>
          <table style="width:100%; margin-bottom: 20px;">
              <tr>
                  <!-- Image on the left with larger size -->
                  <td style="width:40%; vertical-align: top;">
                      <img src="publication/pidnet.png" alt="Prompt Tuning Image" class="publication-image">
                  </td>
                  
                  <!-- Text on the right -->
                  <td style="width:60%; vertical-align: top;">
                      <b style="font-size: 18px; color: #1A5276;">PIDNet: An Efficient Network for Dynamic Pedestrian Intrusion Detection</b><br>
                      <b>Jingchen Sun</b>, Jiming Chen, Tao Chen, Jiayuan Fan, Shibo He<br>
                      <span style="font-style: italic;">ACM Multimedia 2020</span><br>

                      <!-- Links for DOI, PDF, BibTex, Project Webpage -->
                      <a href="https://arxiv.org/abs/2009.00312" target="_blank" class="btn btn-orange">Code</a>
                      <a href="https://arxiv.org/abs/2009.00312" target="_blank" class="btn btn-red">PDF</a>
                  </td>
              </tr>
          </table>
      </li>

      <!-- Repeat for other publications -->
  </ul>

  <h2>Internship</h2>
  <ul style="list-style-type: none; padding: 0;"> <!-- ÁßªÈô§ÈªòËÆ§ÁöÑÂàóË°®Ê†∑ÂºèÔºåÁ°Æ‰øùÂ∑¶ÂØπÈΩê -->
      <li style="display: flex; align-items: flex-start; margin-bottom: 40px;"> <!-- Â¢ûÂä†ÂõæÁâá‰∏éÊñáÂ≠óÁöÑË∑ùÁ¶ªÂíåÂ∑¶ÂØπÈΩê -->
          <!-- NEC Labs Logo -->
          <img src="interns/nec2.png" alt="NEC Labs America" style="width: 180px; margin-right: 60px;"> <!-- ÂõæÁâáÊîæÂ§ß1.2ÂÄçÔºåÂ¢ûÂ§ßÂè≥‰æßÈó¥Ë∑ù -->
          <div style="text-align: left;">
              <b>NEC Labs America</b><br>
              Research Intern<br>
              Princeton, NJ, June 2024/2025 - Aug 2024/2025
          </div>
      </li>

      <li style="display: flex; align-items: flex-start; margin-bottom: 40px;">
          <!-- Nokia Bell Labs Logo -->
          <img src="interns/nokia-bell.png" alt="Nokia Bell Labs" style="width: 180px; margin-right: 60px;"> <!-- ÂõæÁâáÊîæÂ§ß1.2ÂÄçÔºåÂ¢ûÂ§ßÂè≥‰æßÈó¥Ë∑ù -->
          <div style="text-align: left;">
              <b>Nokia Bell Labs</b><br>
              Applied AI Researcher Intern<br>
              Murray Hill, NJ, June 2023 - Aug 2023
          </div>
      </li>

      <li style="display: flex; align-items: flex-start; margin-bottom: 40px;">
          <!-- Bytedance Logo -->
          <img src="interns/bytedance.png" alt="Bytedance" style="width: 180px; margin-right: 60px;"> <!-- ÂõæÁâáÊîæÂ§ß1.2ÂÄçÔºåÂ¢ûÂ§ßÂè≥‰æßÈó¥Ë∑ù -->
          <div style="text-align: left;">
              <b>Bytedance</b><br>
              Computer Vision Intern<br>
              Shanghai, May 2021 - July 2021
          </div>
      </li>
  </ul>

  <h2>Conference Reviewrs</h2>
  <ul>
      <li type="disc">NeurIPS 2024-2025, ICLR 2025-2026, ICML 2025, AISTATS 2025, IJCAI 2025, WACV 2025, ACM MM 2025, ICASSP 2025-2026, IJCNN 2025</li>
  </ul>
  <h2>Journal Reviewrs</h2>
  <ul>
      <li type="disc">Transactions on Machine Learning Research 2024-2025</li>
  </ul>
  <h2>Teaching Asistant</h2>
  <ul>
      <li>CSE 474/574: Introduction to Machine Learning (2024 Spring, 2022 Fall)</li>
      <li>CSE 676: Deep Learning (2022 Spring)</li>
  </ul>

</div>
</body>
